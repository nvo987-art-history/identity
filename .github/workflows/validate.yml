name: ULTRA BRUTAL Site Audit (Enterprise AI + SEO + Security)

on:
  push:
  pull_request:

jobs:
  validate:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      # -----------------------
      # Install tools
      # -----------------------
      - name: Install tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq libxml2-utils curl poppler-utils nodejs npm
          npm install -g lighthouse

      # -----------------------
      # JSON strict
      # -----------------------
      - name: Validate JSON
        run: find . -name "*.json" -type f -exec jq empty {} \;

      # -----------------------
      # XML strict
      # -----------------------
      - name: Validate XML
        run: find . -name "*.xml" -type f -exec xmllint --noout {} \;

      # -----------------------
      # HTML strict
      # -----------------------
      - name: Validate HTML
        run: find . -name "*.html" -type f -exec xmllint --html --noout {} \;

      # -----------------------
      # Required files HARD
      # -----------------------
      - name: Required files
        run: |
          test -f robots.txt
          test -f llms.txt
          test -f sitemap.xml

      # -----------------------
      # Require schema.org
      # -----------------------
      - name: Require schema.org
        run: grep -r "schema.org" . >/dev/null

      # -----------------------
      # SEO tags required
      # -----------------------
      - name: SEO tags
        run: |
          grep -r "<title>" .
          grep -r "description" .
          grep -r "canonical" .

      # -----------------------
      # HTTPS only
      # -----------------------
      - name: Block HTTP links
        run: |
          ! grep -r "http://" . || exit 1

      # -----------------------
      # Empty files FAIL
      # -----------------------
      - name: No empty files
        run: |
          test -z "$(find . -type f -empty -not -path './.git/*')"

      # -----------------------
      # Duplicate filenames FAIL
      # -----------------------
      - name: No duplicates
        run: |
          dup=$(find . -type f -exec basename {} \; | sort | uniq -d)
          test -z "$dup"

      # -----------------------
      # Large files FAIL
      # -----------------------
      - name: Size limit
        run: |
          big=$(find . -type f -size +10M)
          test -z "$big"

      # -----------------------
      # Broken links HARD
      # -----------------------
      - name: Check all links
        run: |
          urls=$(grep -rhoE 'https?://[A-Za-z0-9._~:/?#@!$&()*+,;=%-]+' . | sort -u)
          for u in $urls; do
            code=$(curl -Is -o /dev/null -w "%{http_code}" --max-time 10 "$u")
            echo "$u -> $code"
            [ "$code" = "200" ] || exit 1
          done

      # -----------------------
      # Sitemap links
      # -----------------------
      - name: Check sitemap
        run: |
          urls=$(grep -oP '(?<=<loc>).*?(?=</loc>)' sitemap.xml)
          for u in $urls; do
            code=$(curl -Is -o /dev/null -w "%{http_code}" "$u")
            [ "$code" = "200" ] || exit 1
          done

      # -----------------------
      # Security headers
      # -----------------------
      - name: Security headers
        run: |
          url=$(grep -oP '(?<=<loc>).*?(?=</loc>)' sitemap.xml | head -n1)
          headers=$(curl -Is "$url")
          echo "$headers" | grep -i "strict-transport-security"
          echo "$headers" | grep -i "content-security-policy"

      # -----------------------
      # PDF readable
      # -----------------------
      - name: PDFs readable
        run: |
          find . -name "*.pdf" -type f -exec pdftotext {} /dev/null \;

      # -----------------------
      # SHA256
      # -----------------------
      - name: Checksums
        run: |
          find . -type f -not -path "./.git/*" -exec sha256sum {} \; > checksums.sha256

      # -----------------------
      # AI dataset
      # -----------------------
      - name: AI dataset
        run: |
          mkdir ai_dataset
          find . -type f \( -name "*.json" -o -name "*.xml" -o -name "*.txt" -o -name "*.html" \) -exec cp {} ai_dataset/ \;
          tar -czf ai_dataset.tar.gz ai_dataset

      # -----------------------
      # Lighthouse SEO audit
      # -----------------------
      - name: Lighthouse audit
        run: |
          url=$(grep -oP '(?<=<loc>).*?(?=</loc>)' sitemap.xml | head -n1)
          lighthouse "$url" --chrome-flags="--headless" --only-categories=seo,performance,accessibility --output=json --output-path=report.json

      # -----------------------
      # Upload artifacts
      # -----------------------
      - uses: actions/upload-artifact@v4
        with:
          name: ultra-report
          path: |
            checksums.sha256
            ai_dataset.tar.gz
            report.json 
